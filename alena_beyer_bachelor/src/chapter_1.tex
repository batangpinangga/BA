\chapter{Introduction}
\label{chap:introduction}

\section{Motivation}
% I: Big data is hot topic in companies
In the era of Big Data many companies are faced the problem of how to visualize of large - and continuously growing - volumes of data. While companies such as Google,  Uber and Netflix have already developed data-driven business models \cite{Ryan2016}, other businesses are also discovering the value of data and striving to do "Big Data"\footnote{While out of 1000 surveyed companies only 5.4\% of the firms invested more than \$50MM in Big Data in 2014,  this share rose to 26, 8\% in 2017  \cite{Bean2016}.}. The Big Data Executive Survey 2016 published by NewVantage Partners showed that firms expect the to be greater analytical possibilities with Big Data than with small sized data. Businesses hope that insights gained from Big Data would leverage their market advantage  \cite{Bean2016}. The analysis of Big Data can be performed either by automatic data mining algorithms or by human guided visual analysis. In this regard information visualization plays an important part: it reduces cognitive overload and complexity, and thus supports the data analyst with the visual representation of the analysis.
\par
% I: Big Data is a challenge in visualization
 However,  visual exploration of Big Data is difficult to achieve as large data volumes present a significant challenge to data visualization techniques. There is the need to display billions of database records on a limited screen with a usual screen size of 1600x1200 pixels. With screen limitation problems - such as pixel overlap - visual clutter and information overload arise. Visualization has to overcome the Big Data challenge with appropriate visualization techniques. As the aim of visualization is insight into data,  data visualization must present data in a way so that the reader gains new information from the depicted visualization.
\par
% I: Multi-variate data is 
One important research area for companies is the visualization of multivariate time-oriented data. Even though application areas for business data are manifold, one commonality is the collection and analysis of time-dependent data. Streaming Data is one famous example of time-dependent data as it denotes continually arriving data (marked with a timestamp), which is usually of massive data volume  \cite{Callaghan2002}. As streaming data can appear in various domains such as customer data,  smart manufacturing,  fraud detection,  internet of things or risk management time-oriented data is highly relevant for business. Thus,  companies need to analyze time-oriented data,  usually with the help of data analysis tools. 
\par 
% I: Example of Fault Detection
As an example from the automobile industry: modern  vehicles typically perform ongoing collection of data via the car's sensors. These data sets are then used for various purposes such as error detection. If the motor oil sensor measured that the oil is low a corresponding signal will automatically appear in the electronic instrument cluster. If the mileage is above a determined level the driver is requested to visit the garage. All these error signals are emitted from the error memory in the car. The error memory is a memory which is manually implemented during the vehicle's development. During the development stage engineers are typically not yet familiar with error thresholds. Therefore, they analyze the data sets available to them in order to detect outliers and determine threshold values. In this context,  visual exploration is necessary but challenging. On the one hand the sensors record signals with very high frequency which results in massive amounts of data. On the other hand engineers still need to get familiar with the data. Where large scale visualizations exist, visual exploration can support developers by revealing patterns and outliers. Thus,  the need for scalable visualizations and appropriate tools becomes obvious. 


\section{Research Question}
As time-oriented data is characterized by high volume the question arises whether the tools used in business are able to display big time-oriented data to the business user in an effective manner. The term effectiveness is defined in \ref{effective}. The aim of this work is to approach the research question above by characterizing the data and by studying the business user and different visualization techniques. Based on this analysis requirements for the visualization of large time-oriented data are derived. These requirements are applied in the comparison of a selection of state-of-the-art tools that are used in companies for the analysis of business data.


\section{Structure of this work}
This work is structured in five parts which aim to guide the reader in a logical way through the process of answering the research question. The first part (\ref{concepts}) includes definitions and basic concepts that are used throughout this work.  The chapter that follows (\ref{chap:related Work}) summarises the existing publications that relate to these definitions. The main part of this work is dealt with in the two succeeding sections with the analysis of large time-oriented data (\ref{chap:BIV}) and the tool comparison (\ref{chap:Tools}). The analysis of large time-oriented data is split up into three parts: the data domain,  the user domain and the visualization domain. The data domain covers the question "What is presented?" and studies the particularities of time-oriented data (\ref{data}). The user domain defines psychological findings regarding information visualization  (\ref{perception}) and describes user tasks for time-oriented data (\ref{tasks}). The visualization domain compares visualization techniques for time-oriented data regarding their ability to display large data (\ref{vis}). Based on this analysis a set of important factors is derived which are the basis for the next section: the tool comparison (\ref{chap:Tools}). For this comparison a set of tools has been selected (\ref{tool:selection}) and compared with respect to their ability to display large data. In a next step a classification scheme is developed which is based on the factors of the analysis of the preceding chapter (\ref{tool:classification}). This scheme is then applied to the selected tools.
Based on the tool comparison a conclusion is derived.
As this work only covers one specific part of the visualization of large data the last chapter covers limitations of this work (\ref{limitations}) and resulting future work (\ref{Future Work}).